# ğŸš€ Pipeline Open Data - Acquisition et Enrichissement

# TP2 PIPELINE BIS
**â€” Pipeline d'acquisition et transformation de donnÃ©es**  
*Data Engineering - Open Data Pipeline*

---

## ğŸ“‹ Table des MatiÃ¨res
- [ğŸ¯ Objectifs](#-objectifs)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸ“¦ Installation](#-installation)
- [ğŸš€ Utilisation](#-utilisation)
- [ğŸ§ª Tests](#-tests)
- [ğŸ“Š Exemple de Sortie](#-exemple-de-sortie)
- [ğŸ“ Structure du Projet](#-structure-du-projet)
- [ğŸ“ CritÃ¨res TP ValidÃ©s](#-critÃ¨res-tp-validÃ©s)
- [ğŸ¤– IA Integration](#-ia-integration)
- [ğŸ“ˆ MÃ©triques de QualitÃ©](#-mÃ©triques-de-qualitÃ©)
- [ğŸ”§ DÃ©pendances](#-dÃ©pendances)
- [ğŸ‘¥ Auteurs](#-auteurs)

---

## ğŸ¯ Objectifs

Ce projet implÃ©mente un pipeline de production pour l'acquisition, l'enrichissement et la validation de donnÃ©es Open Data.

### Objectifs pÃ©dagogiques atteints :
1. âœ… **Interroger plusieurs APIs REST Open Data** (OpenFoodFacts + API Adresse)
2. âœ… **Enrichir des donnÃ©es** en croisant plusieurs sources
3. âœ… **GÃ©rer pagination, erreurs et rate limiting**
4. âœ… **ImplÃ©menter un scoring de qualitÃ©** des donnÃ©es
5. âœ… **Transformer et nettoyer** des donnÃ©es avec l'aide de l'IA
6. âœ… **Stocker des donnÃ©es au format Parquet** avec partitionnement
7. âœ… **GÃ©nÃ©rer un rapport de qualitÃ©** automatique
8. âœ… **Construire un pipeline reproductible et testÃ©**

---

## ğŸ—ï¸ Architecture

![alt text](deepseek_mermaid_20251216_416395.svg)
---

### Modules clÃ©s :
- **`fetchers/`** : Acquisition multi-sources avec retry et rate limiting
- **`enricher.py`** : Enrichissement croisÃ© (gÃ©ocodage des magasins)
- **`transformer.py`** : Nettoyage, validation et colonnes dÃ©rivÃ©es
- **`quality.py`** : Scoring qualitÃ© + rapports IA automatisÃ©s
- **`storage.py`** : Stockage Parquet avec partitionnement
- **`main.py`** : Orchestrateur complet avec logging professionnel

---

## ğŸ“¦ Installation

### PrÃ©requis
- Python 3.10+
- [UV](https://github.com/astral-sh/uv) (gestionnaire de paquets rapide)

### Installation rapide


### Cloner le dÃ©pÃ´t
git clone https://github.com/Deep-KALYAN/open-data-ai-models-uv.git
cd tp2-pipeline

#### Installer les dÃ©pendances
uv add httpx pandas duckdb litellm python-dotenv tenacity tqdm pyarrow pydantic pytest

uv sync

#### Configuration API (optionnel)
cp .env.example .env
# Ã‰diter .env avec vos clÃ©s API (Groq, Gemini, etc.)


### DÃ©pendances principales
httpx : RequÃªtes HTTP asynchrones

pandas : Manipulation de donnÃ©es

pydantic : Validation des donnÃ©es

tenacity : Retry automatique

litellm : IntÃ©gration IA multi-fournisseurs

pyarrow : Format Parquet

### ğŸš€ Utilisation
#### Pipeline complet

### Lancer le pipeline complet
uv run python run_pipeline.py --category chocolats --max-items 100

### Options disponibles
uv run python run_pipeline.py --help

### Exemples :
uv run python run_pipeline.py --category biscuits --max-items 50
uv run python run_pipeline.py --category boissons --skip-enrichment --quiet
uv run python run_pipeline.py --category snacks --partition-by categories


### Utilisation programmatique

------------------------------
from pipeline.main import PipelineOrchestrator

orchestrator = PipelineOrchestrator(verbose=True)
stats = orchestrator.run_pipeline(
    category="chocolats",
    max_items=50,
    skip_enrichment=False
)

print(f"Note qualitÃ©: {stats['quality_grade']}")
print(f"Fichier gÃ©nÃ©rÃ©: {stats['output_path']}")


ğŸ§ª Tests

### Tests unitaires
uv run pytest tests/ -v

### Avec couverture de code
uv run pytest tests/ -v --cov=pipeline --cov-report=html

### Tests spÃ©cifiques
uv run pytest tests/test_fetchers.py -v
uv run pytest tests/test_transformer.py -v

### ğŸ“Š Rapport de QualitÃ© des DonnÃ©es
GÃ©nÃ©rÃ© le : 2025-12-16 17:45:37

### ğŸ“ˆ MÃ©triques Globales
| MÃ©trique          | Valeur | Statut |
|----------         |--------|--------|
| Note globale      | C      | âœ… Acceptable |
| ComplÃ©tude        | 92.5%  | âœ… |
| Doublons          | 1.2%   | âœ… |
| GÃ©ocodage rÃ©ussi  | 45.3%  | âš ï¸ |

### ğŸ¤– Recommandations IA
1. AmÃ©liorer le gÃ©ocodage des adresses de magasins
2. ComplÃ©ter les valeurs nutritionnelles manquantes
3. Standardiser les formats de marques
...

### Fichiers gÃ©nÃ©rÃ©s

```text
data/
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ chocolats_raw_20251216_174557.json     # DonnÃ©es brutes
â”œâ”€â”€ processed/
â”‚   â””â”€â”€ chocolats_20251216_174557.parquet     # DonnÃ©es enrichies
â””â”€â”€ reports/
    â””â”€â”€ chocolats_quality_20251216_174557.md  # Rapport qualitÃ©



###    Structure du Projet

tp2-pipeline/
â”œâ”€â”€ pipeline/                   # Code source
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py               # Configuration centralisÃ©e
â”‚   â”œâ”€â”€ models.py               # ModÃ¨les Pydantic (validation)
â”‚   â”œâ”€â”€ fetchers/               # Acquisition multi-sources
â”‚   â”‚   â”œâ”€â”€ base.py             # Classe abstraite (retry, rate limiting)
â”‚   â”‚   â”œâ”€â”€ openfoodfacts.py    # API OpenFoodFacts
â”‚   â”‚   â””â”€â”€ adresse.py          # API Adresse (gÃ©ocodage)
â”‚   â”œâ”€â”€ enricher.py             # Enrichissement croisÃ©
â”‚   â”œâ”€â”€ transformer.py          # Nettoyage et transformation
â”‚   â”œâ”€â”€ quality.py              # Scoring qualitÃ© + IA
â”‚   â”œâ”€â”€ ai_helper.py            # Gestion multi-fournisseurs IA
â”‚   â”œâ”€â”€ storage.py              # Stockage Parquet
â”‚   â””â”€â”€ main.py                 # Orchestrateur principal
â”œâ”€â”€ tests/                      # Tests unitaires
â”‚   â”œâ”€â”€ test_fetchers.py
â”‚   â”œâ”€â”€ test_transformer.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ test_enricher.py
â”œâ”€â”€ data/                       # DonnÃ©es gÃ©nÃ©rÃ©es
â”‚   â”œâ”€â”€ raw/                     # JSON bruts
â”‚   â”œâ”€â”€ processed/               # Parquet enrichis
â”‚   â””â”€â”€ reports/                 # Rapports qualitÃ©
â”œâ”€â”€ notebooks/                  # Exploration (optionnel)
â”œâ”€â”€ pyproject.toml              # DÃ©pendances
â”œâ”€â”€ requirements.txt            # DÃ©pendances (backup)
â”œâ”€â”€ .env.example                # Variables d'environnement
â”œâ”€â”€ run_pipeline.py             # Script d'exÃ©cution
â””â”€â”€ README.md                   # Cette documentation

```
### Description des modules principaux
#### pipeline/config.py
Configuration centralisÃ©e du pipeline :

Chemins des rÃ©pertoires (data/, raw/, processed/, reports/)

Configuration des APIs (URLs, timeouts, rate limits)

Seuils de qualitÃ© (completeness_min, geocoding_score_min, etc.)

ParamÃ¨tres d'acquisition (MAX_ITEMS, BATCH_SIZE)

#### pipeline/models.py
ModÃ¨les de donnÃ©es avec validation Pydantic :

Product : ModÃ¨le d'un produit alimentaire avec validation NutriScore

GeocodingResult : RÃ©sultat de gÃ©ocodage avec score de confiance

QualityMetrics : MÃ©triques de qualitÃ© du dataset avec scoring A-F

#### pipeline/fetchers/
Acquisition robuste avec :

BaseFetcher : Classe abstraite avec retry (tenacity), rate limiting, statistiques

OpenFoodFactsFetcher : RÃ©cupÃ©ration paginÃ©e des produits alimentaires

AdresseFetcher : GÃ©ocodage d'adresses franÃ§aises

#### pipeline/enricher.py
Enrichissement croisÃ© des donnÃ©es :

Extraction d'adresses uniques des produits

Cache de gÃ©ocodage pour Ã©viter les requÃªtes en double

Fusion des donnÃ©es gÃ©ocodÃ©es avec les produits originaux

Statistiques d'enrichissement (taux de succÃ¨s, Ã©checs)

#### pipeline/transformer.py
Pipeline de transformation chaÃ®nable :

Suppression de doublons

Gestion des valeurs manquantes (mÃ©diane, moyenne, zÃ©ro, 'unknown')

Normalisation texte (strip, lowercase, accents)

Filtrage d'outliers (IQR, Z-score)

Colonnes dÃ©rivÃ©es (catÃ©gories de sucre, flags gÃ©ocodÃ©s)

#### pipeline/quality.py
Analyse et scoring de qualitÃ© :

Calcul de mÃ©triques (complÃ©tude, doublons, gÃ©ocodage)

Notation A-F basÃ©e sur des seuils configurables

GÃ©nÃ©ration de rapports Markdown automatisÃ©s

IntÃ©gration IA pour recommandations (Groq, Gemini, Ollama)

#### pipeline/storage.py
Stockage professionnel :

Sauvegarde JSON brut (traÃ§abilitÃ©)

Export Parquet avec compression snappy

Partitionnement optionnel par colonne

Chargement et recherche de fichiers

#### pipeline/main.py
Orchestrateur du pipeline :

Gestion des 5 Ã©tapes du pipeline

Logging structurÃ© avec Ã©moticÃ´nes et timestamps

Gestion d'erreurs avec retry et fallback

Interface CLI complÃ¨te avec argparse




### ğŸ¤– IA Integration
FonctionnalitÃ©s IA
Recommandations de qualitÃ© automatisÃ©es

Analyse du dataset gÃ©nÃ©rÃ©

5 recommandations prioritaires gÃ©nÃ©rÃ©es

Format Markdown professionnel

Suggestions de transformation

Code pandas gÃ©nÃ©rÃ© automatiquement

AdaptÃ© aux caractÃ©ristiques du dataset

Multi-fournisseurs supportÃ©s

Groq (recommandÃ© - rapide et gratuit)

Ollama (local - modÃ¨les Mistral, Llama, etc.)

### Configuration
##### .env
GROQ_API_KEY=gsk_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
##### Ollama utilise l'API locale par dÃ©faut (http://localhost:11434)

### ğŸ”§ DÃ©pendances
#### pyproject.toml
dependencies = [
    "httpx>=0.27.0",        # RequÃªtes HTTP asynchrones (meilleur que requests)
    "pandas>=2.0.0",        # Manipulation de donnÃ©es (DataFrames)
    "pydantic>=2.0.0",      # Validation de donnÃ©es (modÃ¨les)
    "tenacity>=8.2.0",      # Retry automatique (exponentiel backoff)
    "tqdm>=4.66.0",         # Barres de progression (UX)
    "pyarrow>=15.0.0",      # Format Parquet (stockage efficace)
    "litellm>=1.35.0",      # Abstraction multi-fournisseurs IA
    "python-dotenv>=1.0.0", # Variables d'environnement (.env)
    "duckdb>=0.10.0",       # Analyse SQL sur Parquet (optionnel)
]

### DÃ©veloppement & Tests
[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",        # Framework de tests
    "pytest-cov>=4.1.0",    # Couverture de code
]

### APIs externes utilisÃ©es
#### OpenFoodFacts : DonnÃ©es produits alimentaires (gratuit, sans auth)

URL : https://world.openfoodfacts.org/api/v2

Rate limit : 1.5 secondes entre requÃªtes

User-Agent requis

#### API Adresse : GÃ©ocodage franÃ§ais (gratuit, sans auth)

URL : https://api-adresse.data.gouv.fr

Ultra-rapide (< 100ms)

Format GeoJSON avec score de confiance

#### APIs IA (optionnelles) :

Groq : groq/llama-3.3-70b-versatile (rapide, gratuit)



#### ğŸ‘¥ Auteurs
Ã‰tudiant : Deep Kalyan
Formation : Data Engineering - IPSSI
Promotion : 2025
TP : NÂ°2 - Pipeline d'acquisition et transformation de donnÃ©es
Date de rÃ©alisation : DÃ©cembre 2025
DurÃ©e : 5 heures (conforme aux exigences du TP)

#### Contexte pÃ©dagogique
Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre du TP2 de la formation Data Engineering, avec les contraintes suivantes :

DurÃ©e limitÃ©e Ã  5 heures

Livrable : Pipeline Python complet + donnÃ©es enrichies + rapport qualitÃ©

Ã‰valuation individuelle

Respect strict des objectifs pÃ©dagogiques dÃ©finis

#### CompÃ©tences dÃ©montrÃ©es
Data Engineering : Conception et implÃ©mentation de pipelines ETL/ELT

APIs REST : Consommation, pagination, gestion d'erreurs, rate limiting

QualitÃ© des donnÃ©es : MÃ©triques, scoring, recommandations automatisÃ©es

Format Parquet : Stockage column-oriented pour l'analytique

Tests unitaires : Pytest, mocks, couverture de code

IntÃ©gration IA : Utilisation raisonnÃ©e des LLMs pour l'analyse

Documentation : README professionnel, code commentÃ©

#### Remerciements
OpenFoodFacts : Pour leur API ouverte et documentation complÃ¨te

API Adresse (data.gouv.fr) : Pour le service de gÃ©ocodage performant

Groq : Pour leur API IA gratuite et rapide

Enseignants IPSSI : Pour l'encadrement pÃ©dagogique

CommunautÃ© Python : Pour les excellentes bibliothÃ¨ques utilisÃ©es

#### ğŸ“„ License
Ce projet est un travail pÃ©dagogique rÃ©alisÃ© dans le cadre de la formation Data Engineering de l'IPSSI.

Usage : Strictement Ã©ducatif et dÃ©monstratif
DonnÃ©es : Respect des conditions d'utilisation des APIs externes
Code : Libre d'utilisation pour apprentissage, avec mention d'origine

#### Disclaimer
Les APIs externes peuvent avoir des limites d'utilisation

Les clÃ©s API personnelles ne doivent pas Ãªtre commitÃ©es

Ce pipeline est conÃ§u pour des volumes modÃ©rÃ©s de donnÃ©es

Adapter les paramÃ¨tres (rate limits, timeouts) selon vos besoins

### ğŸš¨ DÃ©pannage
#### ProblÃ¨mes courants et solutions
#### 1. ImportError pour les modules
##### RÃ©installer les dÃ©pendances
uv sync --reinstall
##### ou
uv pip install -r requirements.txt
#### 2. Timeout OpenFoodFacts
##### Augmenter le timeout dans config.py
OPENFOODFACTS_CONFIG = APIConfig(
    timeout=120,  # Augmenter de 60 Ã  120 secondes
    # ...
)
#### 3. Erreurs de gÃ©ocodage
VÃ©rifier que les adresses sont en franÃ§ais

Limiter le nombre d'adresses gÃ©ocodÃ©es (max 50 par dÃ©faut)

Les adresses informelles ("Carrefour Paris") ont un score bas

#### 4. Tests qui Ã©chouent
#####  Nettoyer le cache pytest
uv run pytest --cache-clear

#####  ExÃ©cuter tests individuellement pour diagnostiquer
uv run pytest tests/test_fetchers.py::TestOpenFoodFactsFetcher -v

#### 5. ProblÃ¨mes de mÃ©moire avec Pandas
#####  RÃ©duire MAX_ITEMS dans config.py
MAX_ITEMS = 200  # Au lieu de 500